{
    "docs": [
        {
            "location": "/",
            "text": "PARAMSHAKTI  Supercomputing Facilty\n\n\nIntroduction\n\n\nThis documentation summarizes the user manual for the PARAM SHAKTI Supercomputing facility at IIT-KGP Kharagpur. \nIt covers a wide range of topics from a detailed description of the hardware information and about the basic production environment such as how-to login, information about submitting jobs, etc.\nThe supercomputer PARAM SHAKTI is based on a heterogeneous and hybrid configuration of Intel Xeon Skylake processors,NVIDIA Tesla V100. The system was designed and implemented by HPC Technologies team, Centre for Development of Advanced Computing (C-DAC).\nIt consists of 2 Master nodes, 8 Login nodes, 6 Service nodes, 2 Firewall nodes , 2 Management nodes and 442 (CPU+GPU) nodes with total peak computing capacity of 1.63 (CPU+GPU) PFLOPS performance.\n\n\nSystem Architecture and Configuration\n\n\nPARAM SHAKTI systems are based onIntel Xeon SKL G-6148, NVIDIA Tesla V100 with total peak performance of 1.63 PFLOPS.\n The cluster consists of compute nodes connected with Mellanox (ERD) infiniBand interconnect network.\n The system uses the Lustre parallel file system.\n\n\n\n\n                           Figure :(1). PARAM SHAKTI Architecture Daigram\n\n\n\nSystem Hardware Specifications\n\n\nTotal number of nodes: \n 442 (384 + 22 + 36)\n\n\n   Master nodes: 2\n\n   Login nodes: 8\n\n   Service nodes: 10\n\n   CPU nodes: 420\n*   GPU accelerated nodes: 22\n\n\n\n\n\n\nMaster Nodes: 2\n\n\n Master Nodes  : 2*2*  Intel Xeon SKL G-6148\n Cores =40, 2.4 Ghz ,Total Cores = 80 cores\n Memory= 384 GB, Total Memory  = 768 GB\n HDD = 900 GB\n\n\n\n\n\n\n\nLogin Nodes: 8\n\n\nLogin Nodes  : 8\n2\n Intel Xeon SKL G-6148\n   Cores = 40, 2.4 Ghz ,Total Cores = 320 cores\n   Memory= 384 GB ,Total Memory = 3072 GB\n   HDD = 900 GB\n\n\n\n\n\n\nCPU Compute Nodes: 420 \n\n\n*  CPU only Compute Nodes  : 384*2* Intel Xeon SKL G-6148\n   Cores = 40, 2.4GHz ,Total Cores = 15360 cores\n   Memory= 192 GB, DDR4 2666 MHz, Total Memory=737280 GB\n   SSD = 480 GB (local scratch) per node\n\n\n\n\n\nCPU only Compute Nodes High memory  : 36\n2\n Intel Xeon SKL G-6148\n      Cores = 40, 2.4GHz , Total Cores = 1440 cores\n      Memory= 768 GB, DDR4 2666 MHz , Total Memory=27648 GB\n      SSD = 480 GB (local scratch) per node \n\n\n\n\n\n\n\n\nGPU Compute Nodes: 22  \n\n\n\n\n\n\nGPU Compute Nodes : 22\n 2\nnVidia V100\n   CPU Cores = 40, 2.4GHz   Total CPU Cores = 880 \n   Memory= 192 GB, DDR4 2666 MHz    Total Memory= 4224 GB \n   SSD= 480 GB (local scratch) per node\n\n\nGPU Cores per node= 2*5120= 10240   \nGPU Memory = 16 GB HBM2 per nVidia V100\n\n\n\n\n\n\n\nStorage File System\n\n\n*Based on Lustre parallel file system\n\n\n\n\n\nTotal useable capacity 2.1 PiB primary storage and Archival storage 500 TiB\n\n\nThroughput 50GB/s\n\n\n\n\n\n\n\n\nOperating System\n\n\n\n\nOperating system on PARAM SHAKTI is Linux \u2013 CentOS 7.6\n\n\n\n\n\n\n\n\nNetwork Infrastructure\n\n\n      * Computing nodes of PARAM SHAKTI are interconnected by a high-bandwidth, low-latency interconnect network. \n          *Primary Interconnect Network\n           InfiniBand: 100 Gbps\n           InfiniBand is a high-performance communication architecture owned by Mellanox.\n           This communication architecture offers low communication latency, low power consumption and a high throughput.\n           All CPU nodes and GPU nodes are connected via InfiniBand interconnect network.\n\n        * Secondary Interconnect Network\n         Gigabit Ethernet:  1 Gbps\n         Gigabit Ethernet is the interconnect network that is most commonly available.  \n         For Gigabit Ethernet, no additional modules or libraries are required. \n         The Open MPI, MPICH implementations will work over Gigabit Ethernet. \n           Functional Areas Components\n           Base OS                  : CentOS 7.6\n           Architecture         : X86_64\n           Provisioning\n           Cluster Manager          : xCAT  2.14.6\n           Openhpc                  : (ohpc-xCAT 1.3.8)\n           Monitoring Tools         : Nagios, Ganglia, XDMoD\n           Resource Manager         : Slurm\n           I/O Services         : Lustre Client\n           High Speed Interconnects : Mellanox InfiniBand\n           Compiler Families        : GNU (gcc, g++, gfortran)\n           Intel Compiler           : (icc, ifort, icpc)\n           MPI Families         : MVAPICH,  OpenMPI, MPICH\n\n\n\nSoftware Stack\n\n\nSystem Access\n\n\n   *  Accessing the cluster\n   The cluster can be accessed through 8 general login nodes, which allows users to login.\nYou may access login node through ssh.\nThe login node is primary gateway to the rest of the cluster, which has a job scheduler (called SLURM). You may submit jobs to the queue, and they will run when the required resources are available. \nPlease do not run programs directly on login node. Login node is used to submit jobs, transfer data and to compile source code. (If your compilation takes more than a few minutes, you should submit the compilation job into the queue to be run on the cluster.)\n     By default, two directories are available (i.e. /home and /scratch). These directories are available on login node as well as the other nodes on the cluster. /scratch is for temporary data storage, generally used to store data required for running jobs.\n   *  Remote Access\n   *   Using SSH in Windows\n        To access PARAM SHAKTI, you need to \u201cssh\u201d the login server. \n        PuTTY is the most popular open source \u201cssh\u201d client application for Windows, you can Download it from (http://www.putty.org/).\n        Once installed, find the PuTTY application shortcut in your Start Menu, desktop. On clicking the PuTTY icon, The PuTTY Configuration dialog should appear. Locate the \u201cHost Name or IP Address\u201d input Field in the PuTTY Configuration screen. Enter the username along with IP address or Hostname with which you wish to connect. \n        (e.g. [username]@paramshakti.iitkgp.ac.in)\n        Enter your password when prompted, and press Enter.\n   *  Using SSH in Mac or Linux\n      Both Mac and Linux systems provide a built-in SSH client, so there is no need to install any additional package. \n      Open the terminal, connect to an SSH server by typing the following command:\n        ssh[username]@[hostname]\n      For example, to connect to the PARAM SHAKTI Login Node, with the username \n         user1: ssh user1@paramshakti.iitkgp.ac.in\n          You will be prompted for a password, and then will be connected to the server.\n           Password \n           How to change the user password?\n           Use the passwd command to change the password for the user from login node.\n\n\n\nBest Practices for HPC\n\n\n\uf0a7    Do NOT run any job which is longer that few minutes on the login nodes. Login node is for compilation of job. It is best to run the job on computes. \n\uf0a7   It is recommended to go through the beginner\u2019s guide in /home/apps/cdac/samples This should serve as a good starting point for the new users.\n\uf0a7   Use the same compiler to compile different parts/modules/library-dependencies of an application. Using different compilers (e.g. pgcc + icc) to compile different parts of application may cause linking or execution issues.\n\uf0a7   Choosing appropriate compiler switches/flags/options (e.g. \u2013O3) may increase the performance of application substantially (accuracy of output must be verified). Please refer to documentation of compilers (online / docs present inside compiler installation path / man pages etc.)\n\uf0a7   Modules/libraries used for execution should be the same as that used for compilations. This can be specified in the Job submission script.\n\uf0a7   Be aware of the amount of disk space utilized by your job(s). Do an estimate before submitting multiple jobs.\n\uf0a7   Please submit jobs preferably in $SCRATCH. You can back up your results/summaries in your $HOME\n\uf0a7   $SCRATCH is NOT backed up! Please download all your data!\n\n\uf0a7   Before installing any software in your home, ensure that it is from a reliable and safe source. Ransomware is on the rise!\n\uf0a7   Please do not use spaces while creating the directories and files.\n\uf0a7   Please inform hpc-help when you notice something strange - e.g. unexpected slowdowns, files missing/corrupted etc.\n\n\nSlurm Job Submission\n\n\nWhat is SLURM\n\n\n\nSLURM (Simple Linux Utility for Resource Management) is a workload manager that provides a framework for job queues, allocation of compute nodes, and the start and execution of jobs.\nUsing SLURM\nThe cluster compute nodes are available in SLURM partitions. User submits jobs to requisition node resources in a partition. SLURM partitions for general use are \u201cstandard\u201d. A few of the sample commands are given below.\nsinfo   Lists out the status of resources in the system\nsqueue  Lists out the Job information in the system\nsbatch \n    Submitting a job to the scheduler\nscancel \n    Delete a job\n\n\n** Imp Notes:\n  Compilations are done on the login node. Only the execution is scheduled via SLURM on the compute/GPU nodes\n  Upon Submission of a Job script, each job gets a unique Job Id. This can be obtained from the \u2018squeue\u2019 command.\n  The Job Id is also appended to the output and error filenames.\n  Parameters used in SLURM job script\n  The following table gives a few parameters which can be used in the SLURM Job submission script.\n   It should be noted that, most of the parameters are optional. The job flags are used with SBATCH command.\n\n   The syntax for the SLURM directive in a script is \"#SBATCH \n\". A sample SLURM script is given below for your reference. \n\n\nInstalled Applications/Libraries\n\n\nMachine Learning /Deep Learning Applications Development\n\n\nPARAMSHAKTI Support- Quik Guide",
            "title": "Home"
        },
        {
            "location": "/#paramshakti-supercomputing-facilty",
            "text": "",
            "title": "PARAMSHAKTI  Supercomputing Facilty"
        },
        {
            "location": "/#introduction",
            "text": "This documentation summarizes the user manual for the PARAM SHAKTI Supercomputing facility at IIT-KGP Kharagpur. \nIt covers a wide range of topics from a detailed description of the hardware information and about the basic production environment such as how-to login, information about submitting jobs, etc.\nThe supercomputer PARAM SHAKTI is based on a heterogeneous and hybrid configuration of Intel Xeon Skylake processors,NVIDIA Tesla V100. The system was designed and implemented by HPC Technologies team, Centre for Development of Advanced Computing (C-DAC).\nIt consists of 2 Master nodes, 8 Login nodes, 6 Service nodes, 2 Firewall nodes , 2 Management nodes and 442 (CPU+GPU) nodes with total peak computing capacity of 1.63 (CPU+GPU) PFLOPS performance.",
            "title": "Introduction"
        },
        {
            "location": "/#system-architecture-and-configuration",
            "text": "PARAM SHAKTI systems are based onIntel Xeon SKL G-6148, NVIDIA Tesla V100 with total peak performance of 1.63 PFLOPS.\n The cluster consists of compute nodes connected with Mellanox (ERD) infiniBand interconnect network.\n The system uses the Lustre parallel file system.                              Figure :(1). PARAM SHAKTI Architecture Daigram",
            "title": "System Architecture and Configuration"
        },
        {
            "location": "/#system-hardware-specifications",
            "text": "Total number of nodes:   442 (384 + 22 + 36)     Master nodes: 2    Login nodes: 8    Service nodes: 10    CPU nodes: 420\n*   GPU accelerated nodes: 22    Master Nodes: 2   Master Nodes  : 2*2*  Intel Xeon SKL G-6148\n Cores =40, 2.4 Ghz ,Total Cores = 80 cores\n Memory= 384 GB, Total Memory  = 768 GB\n HDD = 900 GB    Login Nodes: 8  Login Nodes  : 8 2  Intel Xeon SKL G-6148\n   Cores = 40, 2.4 Ghz ,Total Cores = 320 cores\n   Memory= 384 GB ,Total Memory = 3072 GB\n   HDD = 900 GB    CPU Compute Nodes: 420   *  CPU only Compute Nodes  : 384*2* Intel Xeon SKL G-6148\n   Cores = 40, 2.4GHz ,Total Cores = 15360 cores\n   Memory= 192 GB, DDR4 2666 MHz, Total Memory=737280 GB\n   SSD = 480 GB (local scratch) per node   CPU only Compute Nodes High memory  : 36 2  Intel Xeon SKL G-6148\n      Cores = 40, 2.4GHz , Total Cores = 1440 cores\n      Memory= 768 GB, DDR4 2666 MHz , Total Memory=27648 GB\n      SSD = 480 GB (local scratch) per node      GPU Compute Nodes: 22      GPU Compute Nodes : 22  2 nVidia V100\n   CPU Cores = 40, 2.4GHz   Total CPU Cores = 880 \n   Memory= 192 GB, DDR4 2666 MHz    Total Memory= 4224 GB \n   SSD= 480 GB (local scratch) per node  GPU Cores per node= 2*5120= 10240   \nGPU Memory = 16 GB HBM2 per nVidia V100    Storage File System  *Based on Lustre parallel file system   Total useable capacity 2.1 PiB primary storage and Archival storage 500 TiB  Throughput 50GB/s     Operating System   Operating system on PARAM SHAKTI is Linux \u2013 CentOS 7.6",
            "title": "System Hardware Specifications"
        },
        {
            "location": "/#network-infrastructure",
            "text": "* Computing nodes of PARAM SHAKTI are interconnected by a high-bandwidth, low-latency interconnect network. \n          *Primary Interconnect Network\n           InfiniBand: 100 Gbps\n           InfiniBand is a high-performance communication architecture owned by Mellanox.\n           This communication architecture offers low communication latency, low power consumption and a high throughput.\n           All CPU nodes and GPU nodes are connected via InfiniBand interconnect network.\n\n        * Secondary Interconnect Network\n         Gigabit Ethernet:  1 Gbps\n         Gigabit Ethernet is the interconnect network that is most commonly available.  \n         For Gigabit Ethernet, no additional modules or libraries are required. \n         The Open MPI, MPICH implementations will work over Gigabit Ethernet. \n           Functional Areas Components\n           Base OS                  : CentOS 7.6\n           Architecture         : X86_64\n           Provisioning\n           Cluster Manager          : xCAT  2.14.6\n           Openhpc                  : (ohpc-xCAT 1.3.8)\n           Monitoring Tools         : Nagios, Ganglia, XDMoD\n           Resource Manager         : Slurm\n           I/O Services         : Lustre Client\n           High Speed Interconnects : Mellanox InfiniBand\n           Compiler Families        : GNU (gcc, g++, gfortran)\n           Intel Compiler           : (icc, ifort, icpc)\n           MPI Families         : MVAPICH,  OpenMPI, MPICH",
            "title": "Network Infrastructure"
        },
        {
            "location": "/#software-stack",
            "text": "",
            "title": "Software Stack"
        },
        {
            "location": "/#system-access",
            "text": "*  Accessing the cluster\n   The cluster can be accessed through 8 general login nodes, which allows users to login.\nYou may access login node through ssh.\nThe login node is primary gateway to the rest of the cluster, which has a job scheduler (called SLURM). You may submit jobs to the queue, and they will run when the required resources are available. \nPlease do not run programs directly on login node. Login node is used to submit jobs, transfer data and to compile source code. (If your compilation takes more than a few minutes, you should submit the compilation job into the queue to be run on the cluster.)\n     By default, two directories are available (i.e. /home and /scratch). These directories are available on login node as well as the other nodes on the cluster. /scratch is for temporary data storage, generally used to store data required for running jobs.\n   *  Remote Access\n   *   Using SSH in Windows\n        To access PARAM SHAKTI, you need to \u201cssh\u201d the login server. \n        PuTTY is the most popular open source \u201cssh\u201d client application for Windows, you can Download it from (http://www.putty.org/).\n        Once installed, find the PuTTY application shortcut in your Start Menu, desktop. On clicking the PuTTY icon, The PuTTY Configuration dialog should appear. Locate the \u201cHost Name or IP Address\u201d input Field in the PuTTY Configuration screen. Enter the username along with IP address or Hostname with which you wish to connect. \n        (e.g. [username]@paramshakti.iitkgp.ac.in)\n        Enter your password when prompted, and press Enter.\n   *  Using SSH in Mac or Linux\n      Both Mac and Linux systems provide a built-in SSH client, so there is no need to install any additional package. \n      Open the terminal, connect to an SSH server by typing the following command:\n        ssh[username]@[hostname]\n      For example, to connect to the PARAM SHAKTI Login Node, with the username \n         user1: ssh user1@paramshakti.iitkgp.ac.in\n          You will be prompted for a password, and then will be connected to the server.\n           Password \n           How to change the user password?\n           Use the passwd command to change the password for the user from login node.",
            "title": "System Access"
        },
        {
            "location": "/#best-practices-for-hpc",
            "text": "\uf0a7    Do NOT run any job which is longer that few minutes on the login nodes. Login node is for compilation of job. It is best to run the job on computes. \n\uf0a7   It is recommended to go through the beginner\u2019s guide in /home/apps/cdac/samples This should serve as a good starting point for the new users.\n\uf0a7   Use the same compiler to compile different parts/modules/library-dependencies of an application. Using different compilers (e.g. pgcc + icc) to compile different parts of application may cause linking or execution issues.\n\uf0a7   Choosing appropriate compiler switches/flags/options (e.g. \u2013O3) may increase the performance of application substantially (accuracy of output must be verified). Please refer to documentation of compilers (online / docs present inside compiler installation path / man pages etc.)\n\uf0a7   Modules/libraries used for execution should be the same as that used for compilations. This can be specified in the Job submission script.\n\uf0a7   Be aware of the amount of disk space utilized by your job(s). Do an estimate before submitting multiple jobs.\n\uf0a7   Please submit jobs preferably in $SCRATCH. You can back up your results/summaries in your $HOME\n\uf0a7   $SCRATCH is NOT backed up! Please download all your data! \n\uf0a7   Before installing any software in your home, ensure that it is from a reliable and safe source. Ransomware is on the rise!\n\uf0a7   Please do not use spaces while creating the directories and files.\n\uf0a7   Please inform hpc-help when you notice something strange - e.g. unexpected slowdowns, files missing/corrupted etc.",
            "title": "Best Practices for HPC"
        },
        {
            "location": "/#slurm-job-submission",
            "text": "What is SLURM  SLURM (Simple Linux Utility for Resource Management) is a workload manager that provides a framework for job queues, allocation of compute nodes, and the start and execution of jobs.\nUsing SLURM\nThe cluster compute nodes are available in SLURM partitions. User submits jobs to requisition node resources in a partition. SLURM partitions for general use are \u201cstandard\u201d. A few of the sample commands are given below.\nsinfo   Lists out the status of resources in the system\nsqueue  Lists out the Job information in the system\nsbatch      Submitting a job to the scheduler\nscancel      Delete a job  ** Imp Notes:\n  Compilations are done on the login node. Only the execution is scheduled via SLURM on the compute/GPU nodes\n  Upon Submission of a Job script, each job gets a unique Job Id. This can be obtained from the \u2018squeue\u2019 command.\n  The Job Id is also appended to the output and error filenames.\n  Parameters used in SLURM job script\n  The following table gives a few parameters which can be used in the SLURM Job submission script.\n   It should be noted that, most of the parameters are optional. The job flags are used with SBATCH command. \n   The syntax for the SLURM directive in a script is \"#SBATCH  \". A sample SLURM script is given below for your reference.",
            "title": "Slurm Job Submission"
        },
        {
            "location": "/#installed-applicationslibraries",
            "text": "",
            "title": "Installed Applications/Libraries"
        },
        {
            "location": "/#machine-learning-deep-learning-applications-development",
            "text": "",
            "title": "Machine Learning /Deep Learning Applications Development"
        },
        {
            "location": "/#paramshakti-support-quik-guide",
            "text": "",
            "title": "PARAMSHAKTI Support- Quik Guide"
        }
    ]
}